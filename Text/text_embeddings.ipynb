{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   clean_tweet_2  label\n",
      "0                                          nigga      4\n",
      "1                                horses retarded      5\n",
      "2  nigga momma youngboy spitting real shit nigga      0\n",
      "3            rt xxsugvngxx ran holy nigga today       1\n",
      "4                       everybody calling nigger      1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"textual_data_an1_cleaned_final.csv\")\n",
    "\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/admin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import nltk\n",
    "\n",
    "# Verify if 'punkt' is available\n",
    "nltk.download('punkt', download_dir='/Users/admin/nltk_data')\n",
    "\n",
    "sentences = data['clean_tweet_2'].dropna().tolist()\n",
    "#tokenized_sentences = [sentence.split() for sentence in sentences]\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokenized_sentences = [tokenizer.tokenize(str(sentence).lower()) for sentence in sentences]\n",
    "\n",
    "#model = Word2Vec(sentences=tokenized_sentences, vector_size=100, window=5, min_count=1, sg=1, epochs=10)\n",
    "\n",
    "#model.save(\"word2vec_model.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'shit': [-1.9590057e-02  4.6684235e-01  7.6496118e-01  7.1472056e-02\n",
      "  7.4470842e-01 -5.0849015e-01  7.2395462e-01  6.5145570e-01\n",
      " -9.7402000e-01 -9.4522440e-01  3.0409154e-01 -8.9401555e-01\n",
      " -8.6123921e-02 -1.3225059e-01 -2.1727771e-01  1.5976471e-01\n",
      "  5.5095989e-01 -3.9442542e-01 -9.4595104e-02 -4.1664425e-01\n",
      "  6.2709384e-02  2.2566906e-01  4.3575704e-02 -1.8051888e-01\n",
      "  1.4805755e-01  2.4423726e-01 -3.7822351e-01 -2.5321850e-01\n",
      " -5.6976473e-01 -5.5646533e-01 -5.4964959e-03 -1.9575089e-01\n",
      " -2.2708194e-01 -8.8444851e-02 -4.9538112e-01  1.2278016e-01\n",
      "  2.0446534e-01 -5.7703382e-03 -1.6811025e-01 -5.7831174e-01\n",
      "  2.0288713e-01  6.7173398e-01  2.1752685e-01  2.2173375e-01\n",
      "  6.0447073e-01 -2.6183934e-03 -3.8476551e-01 -1.7862551e-01\n",
      "  3.2777292e-01  5.9678084e-01  3.7988600e-01 -1.4047587e-01\n",
      " -6.2954897e-01 -1.8691503e-01 -1.0080524e-01  2.6467201e-01\n",
      "  1.2537839e-01 -4.7455542e-04 -3.8506609e-01  6.9666165e-01\n",
      " -5.4413533e-01 -4.0494233e-01  1.5303372e-01 -6.2520456e-01\n",
      " -6.9867522e-01  6.5328069e-02  3.4612224e-01  5.1415068e-01\n",
      " -8.1827506e-02  5.4171830e-01  1.3829793e-01  8.1646889e-02\n",
      "  2.6572245e-01 -3.8418037e-01  5.8188416e-02 -1.0798412e-02\n",
      "  6.7994773e-01  7.0323408e-02  2.0015536e-01 -4.8405853e-01\n",
      " -4.1538857e-02 -1.8969189e-01 -3.3855412e-02  4.3937033e-01\n",
      "  5.4237169e-01 -2.9451381e-02  5.2833676e-01  2.8163478e-01\n",
      " -1.0783368e-01 -1.4738445e-01  5.6753665e-01  4.2622653e-01\n",
      " -2.3356356e-01  4.0020391e-01  4.1521350e-01 -1.4163332e-01\n",
      "  2.7606326e-01 -4.1254703e-02 -2.2212898e-02 -5.8308609e-02]\n",
      "Words similar to 'shit': [('od', 0.6859628558158875), ('ong', 0.6839849352836609), ('youuuuu', 0.6823728084564209), ('df', 0.6798446178436279), ('wifey', 0.6794233322143555), ('shi', 0.6791807413101196), ('stats', 0.6745218634605408), ('fightin', 0.673812747001648), ('baw', 0.6729834675788879), ('snitched', 0.6707852482795715)]\n"
     ]
    }
   ],
   "source": [
    "model = Word2Vec.load('word2vec_model.model')\n",
    "\n",
    "word_vector = model.wv['shit']\n",
    "print(\"Vector for 'shit':\", word_vector)\n",
    "\n",
    "similar_words = model.wv.most_similar('shit')\n",
    "print(\"Words similar to 'shit':\", similar_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words similar to 'hate': [('hates', 0.7087909579277039), ('istg', 0.702265739440918), ('fault', 0.6987146139144897), ('kidnap', 0.692625105381012), ('supportive', 0.6921353340148926)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check most similar words\n",
    "word = \"hate\"\n",
    "similar_words = model.wv.most_similar(word, topn=5)\n",
    "print(f\"Words similar to '{word}': {similar_words}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "#model = FastText(vector_size=100, window=5, min_count=1, sg=1, epochs=10)\n",
    "#model.build_vocab(tokenized_sentences)\n",
    "#model.train(tokenized_sentences, total_examples=len(tokenized_sentences), epochs=10)\n",
    "\n",
    "#model.save(\"fasttext_model.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'hate': [-0.19896089 -0.03855652  0.21756397  0.4290057   0.35268864  0.68681043\n",
      " -0.7992246   0.20737763  0.99332976 -0.63186306 -0.12635213 -0.04417552\n",
      "  0.05359266  0.30471575  0.05943685 -0.5588839   0.7302943  -0.15063119\n",
      " -0.50517595 -0.38255358 -0.0675873   0.6960804  -0.40953735 -0.12459321\n",
      " -0.34085524 -0.3950263  -0.06828337  0.22408985  0.12599343 -0.04847601\n",
      "  0.9342074   0.7227501   0.24040486  0.42846844 -0.08402305  0.5176666\n",
      "  0.5890232   0.8959477  -0.3460574   0.12036419 -0.4729769  -0.23858634\n",
      "  0.6336515  -0.5059421  -0.8992771  -0.0083879  -0.7330792   0.4467388\n",
      "  0.51504976  0.27611798 -0.20217358 -0.50223315  0.20131956  0.27600825\n",
      " -0.00985072 -0.65852374 -0.13819632 -0.09978468  0.40393248  0.07709184\n",
      "  0.2858255  -0.40327787 -0.29199857  0.85438746 -0.17418267  0.8320237\n",
      " -0.03298299 -0.14447047 -0.18150316  0.5372792   0.18773109  0.3217547\n",
      "  0.29163474  0.01376706  0.3485192  -0.02856916  0.37934053 -0.35196486\n",
      "  0.02059063  0.7738005  -0.3987366  -0.34556076 -0.52268577  0.17981549\n",
      "  0.14624427 -0.5121506  -0.6038822   0.42322084  0.00539316  0.80768144\n",
      " -0.7887455   0.05491829  0.24723434  0.26661125  0.3746042  -0.01530444\n",
      "  0.45993578  0.14910318  0.01423992  0.02098648]\n",
      "Similar words: [('hatee', 0.8737678527832031), ('yhate', 0.8697339296340942), ('haterz', 0.8616200685501099), ('hateonme', 0.8575440049171448), ('ihate', 0.85480135679245), ('luv2hate', 0.8517065644264221), ('haten', 0.8365361094474792), ('hatejust', 0.8313389420509338), ('hatersgonnahate', 0.8129274845123291), ('haterswillhate', 0.8066821098327637)]\n"
     ]
    }
   ],
   "source": [
    "model = FastText.load(\"fasttext_model.model\")\n",
    "\n",
    "word_vector = model.wv['hate']\n",
    "print(\"Vector for 'hate':\", word_vector)\n",
    "\n",
    "similar_words = model.wv.most_similar('hate')\n",
    "print(\"Similar words:\", similar_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (60373, 100)\n",
      "Shape of H: (60373, 149822)\n",
      "Feature Matrix Shape (X): (60373, 100)\n",
      "Incidence Matrix Shape (H): (60373, 149822)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from gensim.models import FastText\n",
    "from scipy.sparse import csr_matrix\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load FastText model\n",
    "model = FastText.load(\"fasttext_model.model\")\n",
    "\n",
    "# Sample sentences\n",
    "sentences = data['clean_tweet_2']\n",
    "# Ensure all sentences are strings and handle NaN values\n",
    "sentences = [str(sentence) for sentence in sentences if pd.notnull(sentence)]\n",
    "\n",
    "# Create vocabulary\n",
    "vocab = list(model.wv.index_to_key)\n",
    "# Create a word-to-index dictionary for faster lookups\n",
    "word_to_index = defaultdict(lambda: -1, {word: i for i, word in enumerate(vocab)})\n",
    "\n",
    "# Create embedding matrix (X) - word embeddings\n",
    "embedding_dim = model.vector_size\n",
    "X = np.zeros((len(vocab), embedding_dim), dtype=np.float32)\n",
    "print(\"Shape of X:\", X.shape)\n",
    "for i, word in enumerate(vocab):\n",
    "    X[i] = model.wv[word]\n",
    "\n",
    "# Create hypergraph incidence matrix (H)\n",
    "num_sentences = len(sentences)\n",
    "H = np.zeros((len(vocab), num_sentences), dtype=np.float32)\n",
    "\n",
    "print(\"Shape of H:\", H.shape)\n",
    "\n",
    "for j, sentence in enumerate(sentences):\n",
    "    for word in sentence.split():\n",
    "        i = word_to_index[word]\n",
    "        if i != -1:\n",
    "            H[i, j] = 1\n",
    "\n",
    "# Convert to sparse matrices\n",
    "X_sparse = csr_matrix(X)  # Feature matrix\n",
    "H_sparse = csr_matrix(H)  # Hypergraph incidence matrix\n",
    "\n",
    "print(\"Feature Matrix Shape (X):\", X_sparse.shape)\n",
    "print(\"Incidence Matrix Shape (H):\", H_sparse.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
